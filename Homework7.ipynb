{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPybBhn+CEWTl5IKKb8HJqV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1s5ac/STA365_HW/blob/main/Homework7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Posterior Predictive Distribution for Mixture Models:\n",
        "In mixture models, the posterior predictive distribution represents the probability of future observations given the current model and data. These models are based on the idea that data is generated from several underlying distributions, each representing a subpopulation. To create this distribution:\n",
        "\n",
        "To estimate the mixture weights (probabilities for each subpopulation) and parameters for each subpopulation (such as means and variances), fit the mixture model to the data. To simulate additional data points, use the posterior distributions of these parameters. Based on the mixture weights, select a subpopulation at random for each simulation and then create a data point from the distribution of that subpopulation. Combining the subpopulations' distributions and weighting them according to their estimated mixing weights produces the posterior predictive distribution."
      ],
      "metadata": {
        "id": "bPRcbZU5VM6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Posterior Predictive Distribution in General:\n",
        "The posterior predictive distribution in general Bayesian analysis is the distribution of future observations based on the current data and model. It is created by:\n",
        "\n",
        "Estimating the model parameters' posterior distribution in light of the data that have been observed. By integrating previous assumptions about parameters with the likelihood of the observed data, Bayesian updating is used to achieve this. Obtaining samples from the model parameters' posterior distribution. These samples will produce new data points according to the probability function of the model. The posterior predictive distribution includes All model parameter uncertainties formed from the generated data points.\n"
      ],
      "metadata": {
        "id": "asP20WSmXrAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Bayesian Regression with Missing Data:\n",
        "When performing a Bayesian regression of y on X where X has missing values, you can handle this without discarding incomplete cases by:\n",
        "\n",
        "Treating missing values as latent variables. Set up a Bayesian model where these missing values are part of the parameters to be estimated.\n",
        "Use the observed entries of X to inform the prior distribution of the missing values. This can be guided by the assumption about the missing data mechanism (MCAR, MAR, MNAR).\n",
        "Use a probabilistic programming tool like PyMC to define the likelihood for the observed data, including regression coefficients and the distributions from which the missing values are assumed to be drawn.\n",
        "Run an MCMC algorithm to estimate the posterior distribution of the model parameters and the missing values. During the sampling, the algorithm iteratively imputes the missing values by drawing from their conditional posterior given the current values of the model parameters and the observed data."
      ],
      "metadata": {
        "id": "Oc8cFEMDXuRs"
      }
    }
  ]
}